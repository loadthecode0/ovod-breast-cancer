{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":13718173,"datasetId":8727587,"databundleVersionId":14464781},{"sourceType":"kernelVersion","sourceId":277233593}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n#!/usr/bin/env python3\n\n\"\"\"\ntask_1.py\nZero-Shot Evaluation for Assignment 2 using GroundingDINO\nSupports:\n- Different CSV filenames per dataset\n- Single CSV reused for all datasets\n- Default Kaggle dataset paths\n\"\"\"\n\nimport os\nimport argparse\nfrom tqdm import tqdm\nimport torch\n\n_original_torch_load = torch.load\n\ndef torch_load_patched(*args, **kwargs):\n    kwargs[\"weights_only\"] = False\n    return _original_torch_load(*args, **kwargs)\n\ntorch.load = torch_load_patched\n\nfrom groundingdino.util.inference import load_model, load_image, predict\n\nimport sys\nsys.path.append(\"/kaggle/usr/lib/utils_a2_py\")\n\nfrom utils_a2_py import (\n    DEFAULT_PROMPTS,\n    DEFAULT_THRESHOLDS,\n    download_file,\n    ensure_dir,\n    load_annotations,\n    get_gt_boxes,\n    compute_ap,\n    format_report,\n    GROUNDING_DINO_CONFIG_URL,\n    GROUNDING_DINO_WEIGHTS_URL,\n)\n\n\n# =====================================================\n# CLASS: GroundingDINOEvaluator\n# =====================================================\n\nclass GroundingDINOEvaluator:\n    def __init__(self, device=\"cuda\"):\n        self.device = device\n        self.model = None\n        self.config_path = None\n        self.weights_path = None\n\n    # -------------------------------------------------\n    # (A) Download model files\n    # -------------------------------------------------\n    def download_requirements(self, dst_dir=\"weights\"):\n        ensure_dir(dst_dir)\n\n        self.config_path = os.path.join(dst_dir, \"GroundingDINO_SwinT_OGC.py\")\n        self.weights_path = os.path.join(dst_dir, \"groundingdino_swint_ogc.pth\")\n\n        download_file(GROUNDING_DINO_CONFIG_URL, self.config_path)\n        download_file(GROUNDING_DINO_WEIGHTS_URL, self.weights_path)\n\n        print(self.config_path)\n        print(self.weights_path)\n\n    # -------------------------------------------------\n    # (B) Load model\n    # -------------------------------------------------\n    def load_model(self):\n        print(\"[INFO] Loading GroundingDINO...\")\n        self.model = load_model(self.config_path, self.weights_path, self.device)\n        print(\"[INFO] Model loaded.\")\n\n    # -------------------------------------------------\n    # (C) Evaluate one dataset\n    # -------------------------------------------------\n    def evaluate(self, dataset_path, csv_name, prompt,\n                 box_threshold, text_threshold, iou_threshold=0.5):\n\n        csv_path = os.path.join(dataset_path, csv_name)\n        df = load_annotations(csv_path)\n\n        image_names = df[\"image_name\"].unique()\n        aps = []\n\n        for img_name in tqdm(image_names, desc=f\"Evaluating ({dataset_path})\"):\n\n            img_path = os.path.join(dataset_path, img_name)\n            if not os.path.exists(img_path):\n                continue\n\n            image_source, image = load_image(img_path)\n\n            boxes, logits, phrases = predict(\n                model=self.model,\n                image=image,\n                caption=prompt,\n                box_threshold=box_threshold,\n                text_threshold=text_threshold,\n                device=self.device,\n            )\n\n            pred_boxes = boxes * torch.tensor([\n                image_source.shape[1], image_source.shape[0],\n                image_source.shape[1], image_source.shape[0]\n            ])\n\n            gt_boxes = get_gt_boxes(df, img_name)\n\n            ap = compute_ap(pred_boxes.numpy(), gt_boxes, iou_threshold)\n            aps.append(ap)\n\n        return float(sum(aps) / max(len(aps), 1))\n\n    # -------------------------------------------------\n    # (D) Report\n    # -------------------------------------------------\n    def generate_report(self, results):\n        text = format_report(results)\n        print(text)\n        return text\n\n\n# =====================================================\n# CLI\n# =====================================================\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Task 1 Zero-Shot Evaluation\")\n\n    parser.add_argument(\n        \"--dataset_paths\",\n        nargs=\"+\",\n        required=False,\n        default=[\n            \"/kaggle/input/ovod-datasets/dataset_A/dataset_A/test\",\n            \"/kaggle/input/ovod-datasets/dataset_B/dataset_B/test\",\n            \"/kaggle/input/ovod-datasets/dataset_C/dataset_C/test\"\n        ],\n        help=\"Paths to datasets (each dataset/test folder).\"\n    )\n\n    parser.add_argument(\n        \"--wts_dir\",\n        nargs=\"+\",\n        required=False,\n        default=\"/kaggle/working/weights/\",\n        help=\"Paths to datasets (each dataset/test folder).\"\n    )\n\n    parser.add_argument(\n        \"--annotations\",\n        nargs=\"+\",\n        required=False,\n        default=[\n            \"test.csv\",\n            \"test_updated.csv\",\n            \"test.csv\"\n        ],\n        help=\"One CSV or one per dataset.\"\n    )\n\n    parser.add_argument(\n        \"--prompts\",\n        nargs=\"*\",\n        default=None,\n        help=\"One prompt or one per dataset.\"\n    )\n\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n\n    args = parser.parse_args()\n\n    evaluator = GroundingDINOEvaluator(device=args.device)\n\n    # A: download\n    # evaluator.download_requirements(args.wts_dir)\n\n    # B: load model\n    evaluator.load_model()\n\n    # Annotation logic\n    if len(args.annotations) == 1:\n        annotations_list = [args.annotations[0]] * len(args.dataset_paths)\n    elif len(args.annotations) == len(args.dataset_paths):\n        annotations_list = args.annotations\n    else:\n        raise ValueError(\"Provide ONE CSV or ONE CSV PER dataset.\")\n\n    # Prompt logic\n    if args.prompts and len(args.prompts) == len(args.dataset_paths):\n        prompts = args.prompts\n    elif args.prompts and len(args.prompts) == 1:\n        prompts = [args.prompts[0]] * len(args.dataset_paths)\n    else:\n        prompts = [\n            DEFAULT_PROMPTS.get(chr(ord(\"A\") + i), [\"a malignant tumor\"])[0]\n            for i in range(len(args.dataset_paths))\n        ]\n\n    # Evaluate each dataset\n    results = {}\n\n    for i, dpath in enumerate(args.dataset_paths):\n        ds_name = f\"Dataset_{chr(ord('A') + i)}\"\n        ap = evaluator.evaluate(\n            dataset_path=dpath,\n            csv_name=annotations_list[i],\n            prompt=prompts[i],\n            box_threshold=DEFAULT_THRESHOLDS[\"box_threshold\"],\n            text_threshold=DEFAULT_THRESHOLDS[\"text_threshold\"],\n        )\n\n        results[ds_name] = {\n            \"prompt\": prompts[i],\n            \"box_threshold\": DEFAULT_THRESHOLDS[\"box_threshold\"],\n            \"text_threshold\": DEFAULT_THRESHOLDS[\"text_threshold\"],\n            \"ap\": ap,\n        }\n\n    evaluator.generate_report(results)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"1c70604c-4a48-4c99-ab9e-e45bc2dfa34a","_cell_guid":"81213e2e-b726-4bd0-aa4b-2a63ce69ead9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}